{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549367/549367 [00:05<00:00, 97335.70it/s] \n",
      "100%|██████████| 9842/9842 [00:00<00:00, 131429.22it/s]\n",
      "100%|██████████| 9824/9824 [00:00<00:00, 135284.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from lib.vectorize import vectorize\n",
    "ddict = vectorize(sent_size=50)\n",
    "vocab_size = len(list(ddict['word2index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(ddict['train_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MatchLSTM(nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size, class_size):\n",
    "        super(MatchLSTM, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size    # E\n",
    "        self.vocab_size     = vocab_size        # V\n",
    "               \n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)        #VxE -> #E\n",
    "        \n",
    "        self.premise_lstm    = nn.LSTMCell(embedding_size, embedding_size)\n",
    "        self.hypothesis_lstm = nn.LSTMCell(embedding_size, embedding_size)\n",
    "        self.match_lstm      = nn.LSTMCell(embedding_size, embedding_size)\n",
    "        \n",
    "        self.attend_premise    = nn.Linear(embedding_size, embedding_size, bias=False)\n",
    "        self.attend_hypothesis = nn.Linear(embedding_size, embedding_size, bias=False)\n",
    "        self.attend_state      = nn.Linear(embedding_size, embedding_size, bias=False)\n",
    "        self.attend_match      = nn.Linear(embedding_size, embedding_size, bias=False)\n",
    "        \n",
    "        self.scale = nn.Linear(embedding_size, 1)\n",
    "        self.classify = nn.Linear(embedding_size, class_size)\n",
    "        \n",
    "        self.print_sizes = True\n",
    "        \n",
    "    def initial_hidden_state(self):\n",
    "        return Variable(torch.zeros([1, self.embedding_size]))\n",
    "        \n",
    "    def printsize(tensor):\n",
    "        if self.print_sizes:\n",
    "            print(tensor.size())\n",
    "            \n",
    "    def forward(self, premise, hypothesis):\n",
    "        print('premise:{}'.format(premise.size()))\n",
    "        print('hypothesis:{}'.format(hypothesis.size()))\n",
    "        premise_emb    = self.embed(premise)                           #PlxH\n",
    "        hypothesis_emb = self.embed(hypothesis)                        #HlXH\n",
    "        \n",
    "        print('premise_emb:{}'.format(premise_emb.size()))\n",
    "        print('hypothesis_emb:{}'.format(hypothesis_emb.size()))\n",
    "        \n",
    "        hidden_state = self.initial_hidden_state()\n",
    "        cell_state   = self.initial_hidden_state()\n",
    "        print('hidden_state:{}'.format(hidden_state.size()))\n",
    "        hypothesis_states = []                                              #HlxH\n",
    "        for h in hypothesis_emb:\n",
    "            hidden_state, cell_state = self.hypothesis_lstm(h.view([1, -1]), \n",
    "                                                            (hidden_state, cell_state))\n",
    "            hypothesis_states.append(hidden_state)\n",
    "                \n",
    "        hidden_state = self.initial_hidden_state()\n",
    "        cell_state   = self.initial_hidden_state()\n",
    "        premise_states = []                                                 #PlxH\n",
    "        for p in premise_emb:\n",
    "            hidden_state, cell_state = self.hypothesis_lstm(p.view([1,-1]),\n",
    "                                                            (hidden_state, cell_state))\n",
    "            premise_states.append(hidden_state)\n",
    "            \n",
    "        premise_states = torch.stack(premise_states).squeeze(1)                     #PlXH        \n",
    "        print('premise_states:{}'.format(premise_states.size()))\n",
    "\n",
    "        hidden_state = self.initial_hidden_state()\n",
    "        cell_state   = self.initial_hidden_state()\n",
    "        for h in hypothesis_states:            \n",
    "            hattn = self.attend_hypothesis(h)                               #1xH\n",
    "            print('hattn:{}'.format(hattn.size()))\n",
    "            \n",
    "            pattn = self.attend_premise(premise_states)                     #PlxH\n",
    "            print('pattn:{}'.format(pattn.size()))\n",
    "            \n",
    "            mattn = self.attend_match(hidden_state)                         #1xH\n",
    "            print('mattn:{}'.format(mattn.size()))\n",
    "            \n",
    "            attn = F.softmax(self.scale( hattn.expand_as(pattn)             #PlxH -> scale ->PlX1\n",
    "                                        + pattn\n",
    "                                        + mattn.expand_as(pattn))\n",
    "                            )                                   \n",
    "            print('attn:{}'.format(attn.size()))\n",
    "            \n",
    "            attn = torch.mm(attn.t(), premise_states)                      #1xPl * PlxH -> 1xH \n",
    "            print('attn:{}'.format(attn.size()))\n",
    "            \n",
    "            attn_hidden_mat = torch.cat([attn, h])                         # HXH\n",
    "            print('attn_hidden_mat:{}'.format(attn_hidden_mat.size()))\n",
    "            \n",
    "            hidden_state, cell_state = self.match_lstm(attn_hidden_mat, \n",
    "                                                       (hidden_state, cell_state))\n",
    "            print('hidden_state:{}'.format(hidden_state.size()))\n",
    "            \n",
    "        attended_match_state = hidden_state\n",
    "        return F.log_softmax(self.classify(attended_match_state))\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_one_hot(length, index):\n",
    "    a = np.zeros([length])\n",
    "    a[index] = 1\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "def train(epochs, model,  train_batches, print_every = 100):\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.1)\n",
    "    for epoch in range(epochs+1):\n",
    "        for batch, sample in enumerate(train_batches):\n",
    "            premise = Variable(torch.LongTensor(sample[0]))\n",
    "            hypothesis = Variable(torch.LongTensor(sample[1]))\n",
    "            judgements = Variable(torch.LongTensor(sample[3]))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model(premise, hypothesis)\n",
    "            loss = F.nll_loss(predictions, judgements)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            #print([i for i in model.parameters()])\n",
    "            print('epoch: {}\\t\\t -- loss: {}'.format(epoch, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = ddict['train_data'][0]\n",
    "premise = sample[0]\n",
    "hypothesis = sample[1]\n",
    "judgement = sample[2]\n",
    "judgements = sample[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7cfa3c449997>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m#Premise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m#Hypotheses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-6081c117dedd>\u001b[0m in \u001b[0;36mcreate_one_hot\u001b[0;34m(length, index)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_data = ddict['train_data']\n",
    "\n",
    "\"\"\"\n",
    "for i in range(50):\n",
    "    for j in range(len(train_data[i])):\n",
    "        print(train_data[i][j])\n",
    "        print('--')\n",
    "    print('==================')\n",
    "\"\"\"\n",
    "for sample in train_data[20:30]:\n",
    "    for i in range(len(sample[0])):   #Premise\n",
    "        sample[0][i] = create_one_hot(vocab_size, sample[0][i])\n",
    "        \n",
    "    for i in range(len(sample[1])):   #Hypotheses\n",
    "        sample[1][i] = create_one_hot(vocab_size, sample[1][i])\n",
    "\n",
    "    sample[2] = create_one_hot(3, sample[2])         #Judgement\n",
    "    \n",
    "    sample[0] = torch.Tensor(sample[0])\n",
    "    sample[1] = torch.Tensor(sample[1])\n",
    "    sample[2] = torch.Tensor(sample[2])\n",
    "\n",
    "print(sample[0].size())\n",
    "print(sample[1].size())\n",
    "print(sample[2].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MatchLSTM(30, vocab_size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train(10, model, ddict['train_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
