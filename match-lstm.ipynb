{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549367/549367 [00:05<00:00, 94687.19it/s] \n",
      "100%|██████████| 9842/9842 [00:00<00:00, 90806.65it/s]\n",
      "100%|██████████| 9824/9824 [00:00<00:00, 66129.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from lib.vectorize import vectorize\n",
    "ddict = vectorize(sent_size=50)\n",
    "vocab_size = len(list(ddict['word2index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549367"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ddict['train_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MatchLSTM(nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size, class_size):\n",
    "        super(MatchLSTM, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size    # E\n",
    "        self.vocab_size     = vocab_size        # V\n",
    "               \n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)        #VxE -> #E\n",
    "        \n",
    "        self.premise_lstm    = nn.LSTMCell(embedding_size, embedding_size)\n",
    "        self.hypothesis_lstm = nn.LSTMCell(embedding_size, embedding_size)\n",
    "        self.match_lstm      = nn.LSTMCell(embedding_size, embedding_size)\n",
    "        \n",
    "        self.attend_premise    = nn.Linear(embedding_size, embedding_size, bias=False)\n",
    "        self.attend_hypothesis = nn.Linear(embedding_size, embedding_size, bias=False)\n",
    "        self.attend_state      = nn.Linear(embedding_size, embedding_size, bias=False)\n",
    "        self.attend_match      = nn.Linear(embedding_size, embedding_size, bias=False)\n",
    "        \n",
    "        self.scale = nn.Linear(embedding_size, 1)\n",
    "        self.merge_attention = nn.Linear(2*embedding_size, embedding_size)\n",
    "        self.classify = nn.Linear(embedding_size, class_size)\n",
    "        \n",
    "        self.print_sizes = True\n",
    "        \n",
    "    def initial_hidden_state(self):\n",
    "        return Variable(torch.zeros([1, self.embedding_size]))\n",
    "        \n",
    "    def printsize(tensor):\n",
    "        if self.print_sizes:\n",
    "            print(tensor.size())\n",
    "            \n",
    "    def forward(self, premise, hypothesis):\n",
    "        #print('premise:{}'.format(premise.size()))\n",
    "        #print('hypothesis:{}'.format(hypothesis.size()))\n",
    "        premise_emb    = self.embed(premise)                           #PlxH\n",
    "        hypothesis_emb = self.embed(hypothesis)                        #HlXH\n",
    "        \n",
    "        #print('premise_emb:{}'.format(premise_emb.size()))\n",
    "        #print('hypothesis_emb:{}'.format(hypothesis_emb.size()))\n",
    "        \n",
    "        hidden_state = self.initial_hidden_state()\n",
    "        cell_state   = self.initial_hidden_state()\n",
    "        #print('hidden_state:{}'.format(hidden_state.size()))\n",
    "        hypothesis_states = []                                              #HlxH\n",
    "        for h in hypothesis_emb:\n",
    "            hidden_state, cell_state = self.hypothesis_lstm(h.view([1, -1]), \n",
    "                                                            (hidden_state, cell_state))\n",
    "            hypothesis_states.append(hidden_state)\n",
    "                \n",
    "        hidden_state = self.initial_hidden_state()\n",
    "        cell_state   = self.initial_hidden_state()\n",
    "        premise_states = []                                                 #PlxH\n",
    "        for p in premise_emb:\n",
    "            hidden_state, cell_state = self.hypothesis_lstm(p.view([1,-1]),\n",
    "                                                            (hidden_state, cell_state))\n",
    "            premise_states.append(hidden_state)\n",
    "            \n",
    "        premise_states = torch.stack(premise_states).squeeze(1)                     #PlXH        \n",
    "        #print('premise_states:{}'.format(premise_states.size()))\n",
    "\n",
    "        hidden_state = self.initial_hidden_state()\n",
    "        cell_state   = self.initial_hidden_state()\n",
    "        for h in hypothesis_states:            \n",
    "            hattn = self.attend_hypothesis(h)                               #1xH\n",
    "            #print('hattn:{}'.format(hattn.size()))\n",
    "            \n",
    "            pattn = self.attend_premise(premise_states)                     #PlxH\n",
    "            #print('pattn:{}'.format(pattn.size()))\n",
    "            \n",
    "            mattn = self.attend_match(hidden_state)                         #1xH\n",
    "            #print('mattn:{}'.format(mattn.size()))\n",
    "            \n",
    "            attn = F.softmax(self.scale( hattn.expand_as(pattn)             #PlxH -> scale ->PlX1\n",
    "                                        + pattn\n",
    "                                        + mattn.expand_as(pattn))\n",
    "                            )                                   \n",
    "            #print('attn:{}'.format(attn.size()))\n",
    "            \n",
    "            attn = torch.mm(attn.t(), premise_states)                      #1xPl * PlxH -> 1xH \n",
    "            #print('attn:{}'.format(attn.size()))\n",
    "            \n",
    "            attn_hidden_mat = self.merge_attention(torch.cat([attn, h], 1))                         # HXH\n",
    "            #print('attn_hidden_mat:{}'.format(attn_hidden_mat.size()))\n",
    "            \n",
    "            #print('hidden_state:{}'.format(hidden_state.size()))\n",
    "\n",
    "            hidden_state, cell_state = self.match_lstm(attn_hidden_mat, \n",
    "                                                       (hidden_state, cell_state))\n",
    "            \n",
    "            #print('hidden_state:{}'.format(hidden_state.size()))\n",
    "        attended_match_state = hidden_state\n",
    "        return F.log_softmax(self.classify(attended_match_state))\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def create_one_hot(length, index):\n",
    "    a = np.zeros([length])\n",
    "    a[index] = 1\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "def train(epochs, model,  train_batches, print_every = 100):\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.1)\n",
    "    for epoch in range(epochs+1):\n",
    "        for batch, sample in enumerate(train_batches):\n",
    "            premise = Variable(torch.LongTensor(sample[0]))\n",
    "            hypothesis = Variable(torch.LongTensor(sample[1]))\n",
    "            #judgements = Variable(torch.from_numpy(\n",
    "            #    create_one_hot(3, sample[2])).long().view([1,-1]))\n",
    "            judgements = Variable(torch.LongTensor([sample[2]]))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model(premise, hypothesis)\n",
    "            #print(predictions.size(), judgements.size())\n",
    "            loss = F.nll_loss(predictions, judgements)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch % print_every == 0:\n",
    "                #print([i for i in model.parameters()])\n",
    "                print('epoch: {}\\tbatch: {}\\t -- loss: {}'.format(epoch, batch, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = ddict['train_data'][0]\n",
    "premise = sample[0]\n",
    "hypothesis = sample[1]\n",
    "judgement = sample[2]\n",
    "judgements = sample[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MatchLSTM(3, vocab_size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\tbatch: 0\t -- loss: 1.1436717510223389\n",
      "epoch: 0\tbatch: 100\t -- loss: 1.0857354402542114\n",
      "epoch: 0\tbatch: 200\t -- loss: 1.0953764915466309\n",
      "epoch: 0\tbatch: 300\t -- loss: 1.1129271984100342\n",
      "epoch: 0\tbatch: 400\t -- loss: 1.114245057106018\n",
      "epoch: 0\tbatch: 500\t -- loss: 1.1073307991027832\n",
      "epoch: 0\tbatch: 600\t -- loss: 1.1390821933746338\n",
      "epoch: 0\tbatch: 700\t -- loss: 1.1181352138519287\n",
      "epoch: 0\tbatch: 800\t -- loss: 1.0912469625473022\n",
      "epoch: 0\tbatch: 900\t -- loss: 1.124155879020691\n",
      "epoch: 0\tbatch: 1000\t -- loss: 1.0970820188522339\n",
      "epoch: 0\tbatch: 1100\t -- loss: 1.1396733522415161\n",
      "epoch: 0\tbatch: 1200\t -- loss: 1.1130518913269043\n",
      "epoch: 0\tbatch: 1300\t -- loss: 1.0947747230529785\n",
      "epoch: 0\tbatch: 1400\t -- loss: 1.1371382474899292\n",
      "epoch: 0\tbatch: 1500\t -- loss: 1.0769662857055664\n",
      "epoch: 0\tbatch: 1600\t -- loss: 1.1039656400680542\n",
      "epoch: 0\tbatch: 1700\t -- loss: 1.1066042184829712\n",
      "epoch: 0\tbatch: 1800\t -- loss: 1.0910228490829468\n",
      "epoch: 0\tbatch: 1900\t -- loss: 1.1416176557540894\n",
      "epoch: 0\tbatch: 2000\t -- loss: 1.1138108968734741\n",
      "epoch: 0\tbatch: 2100\t -- loss: 1.0946658849716187\n",
      "epoch: 0\tbatch: 2200\t -- loss: 1.1147770881652832\n",
      "epoch: 0\tbatch: 2300\t -- loss: 1.1153050661087036\n",
      "epoch: 0\tbatch: 2400\t -- loss: 1.1143271923065186\n",
      "epoch: 0\tbatch: 2500\t -- loss: 1.122847080230713\n"
     ]
    }
   ],
   "source": [
    "train(10, model, ddict['train_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
